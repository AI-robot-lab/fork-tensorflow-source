# Przewodnik TensorFlow dla Robota Unitree G1 EDU-U6

## Spis tre≈õci
1. [Wprowadzenie](#wprowadzenie)
2. [Architektura systemu](#architektura-systemu)
3. [Konfiguracja ≈õrodowiska](#konfiguracja-≈õrodowiska)
4. [Praktyczne zastosowania](#praktyczne-zastosowania)
5. [Przyk≈Çadowe projekty](#przyk≈Çadowe-projekty)
6. [RozwiƒÖzywanie problem√≥w](#rozwiƒÖzywanie-problem√≥w)

## Wprowadzenie

### O robocie Unitree G1 EDU-U6

Unitree G1 EDU-U6 to zaawansowany robot humanoidalny zaprojektowany do cel√≥w edukacyjnych i badawczych. Posiada:
- **Kamery** - do percepcji wizualnej (widzenie komputerowe)
- **Mikrofony** - do rozpoznawania mowy
- **Manipulatory** - do interakcji z obiektami
- **Sensory** - do nawigacji i r√≥wnowa≈ºenia

### Dlaczego TensorFlow?

TensorFlow jest idealnym wyborem dla robota G1, poniewa≈º:
- **Wydajno≈õƒá** - Optymalizowany dla r√≥≈ºnych platform (CPU, GPU, TPU)
- **TensorFlow Lite** - Wersja dla urzƒÖdze≈Ñ embedded, idealna dla robotyki
- **Gotowe modele** - Przedtrenowane sieci neuronowe do natychmiastowego u≈ºycia
- **Wsparcie spo≈Çeczno≈õci** - Szeroka baza wiedzy i przyk≈Çad√≥w

## Architektura systemu

### Schemat integracji TensorFlow z robotem G1

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                   Robot Unitree G1 EDU-U6                ‚îÇ
‚îÇ                                                           ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ
‚îÇ  ‚îÇ   Kamery    ‚îÇ  ‚îÇ   Mikrofony  ‚îÇ  ‚îÇ    Sensory      ‚îÇ ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ
‚îÇ         ‚îÇ                ‚îÇ                   ‚îÇ          ‚îÇ
‚îÇ         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò          ‚îÇ
‚îÇ                          ‚îÇ                              ‚îÇ
‚îÇ                          ‚ñº                              ‚îÇ
‚îÇ         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê              ‚îÇ
‚îÇ         ‚îÇ   TensorFlow / TensorFlow Lite ‚îÇ              ‚îÇ
‚îÇ         ‚îÇ                                ‚îÇ              ‚îÇ
‚îÇ         ‚îÇ  ‚Ä¢ Rozpoznawanie obraz√≥w       ‚îÇ              ‚îÇ
‚îÇ         ‚îÇ  ‚Ä¢ Przetwarzanie mowy          ‚îÇ              ‚îÇ
‚îÇ         ‚îÇ  ‚Ä¢ Podejmowanie decyzji        ‚îÇ              ‚îÇ
‚îÇ         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò              ‚îÇ
‚îÇ                      ‚îÇ                                  ‚îÇ
‚îÇ                      ‚ñº                                  ‚îÇ
‚îÇ         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                  ‚îÇ
‚îÇ         ‚îÇ  Sterowanie robotem        ‚îÇ                  ‚îÇ
‚îÇ         ‚îÇ  ‚Ä¢ Manipulatory            ‚îÇ                  ‚îÇ
‚îÇ         ‚îÇ  ‚Ä¢ Nawigacja               ‚îÇ                  ‚îÇ
‚îÇ         ‚îÇ  ‚Ä¢ Interakcja              ‚îÇ                  ‚îÇ
‚îÇ         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Przep≈Çyw danych

1. **Wej≈õcie** - Dane z kamer i mikrofon√≥w
2. **Przetwarzanie** - Modele TensorFlow analizujƒÖ dane
3. **Decyzja** - Wyniki analizy okre≈õlajƒÖ akcje
4. **Wyj≈õcie** - Robot wykonuje odpowiednie ruchy i dzia≈Çania

## Konfiguracja ≈õrodowiska

### Wymagania sprzƒôtowe

**Minimalne:**
- Procesor: 4-rdzeniowy CPU
- RAM: 8 GB
- Dysk: 20 GB wolnego miejsca

**Zalecane dla treningu modeli:**
- GPU: NVIDIA z obs≈ÇugƒÖ CUDA (np. RTX 3060 lub lepszy)
- RAM: 16 GB lub wiƒôcej
- Dysk: SSD z 50 GB wolnego miejsca

### Instalacja na systemie robota

```bash
# 1. Aktualizacja systemu
sudo apt update && sudo apt upgrade -y

# 2. Instalacja Pythona i pip (je≈õli nie ma)
sudo apt install python3 python3-pip -y

# 3. Instalacja TensorFlow Lite (lekka wersja dla robotyki)
pip3 install tflite-runtime

# 4. Lub pe≈Çna wersja TensorFlow (je≈õli wystarczajƒÖca moc obliczeniowa)
pip3 install tensorflow

# 5. Dodatkowe biblioteki dla robotyki
pip3 install numpy opencv-python pillow
```

### Testowanie instalacji

```python
# test_tensorflow.py
import tensorflow as tf
print("Wersja TensorFlow:", tf.__version__)
print("GPU dostƒôpne:", tf.config.list_physical_devices('GPU'))

# Test prostej operacji
a = tf.constant([1, 2, 3])
b = tf.constant([4, 5, 6])
print("Test operacji:", tf.add(a, b).numpy())
```

```bash
python3 test_tensorflow.py
```

## Praktyczne zastosowania

### 1. Rozpoznawanie obiekt√≥w (Object Detection)

**Cel:** Robot rozpoznaje przedmioty w swoim otoczeniu.

**Przypadki u≈ºycia:**
- Identyfikacja obiekt√≥w do podniesienia
- Rozpoznawanie narzƒôdzi
- Wykrywanie ludzi i przeszk√≥d

**Implementacja:**

```python
# object_detection_for_g1.py
"""
Rozpoznawanie obiekt√≥w dla robota Unitree G1
Wykorzystuje przedtrenowany model Inception V3
"""

import tensorflow as tf
import numpy as np
from PIL import Image

# Krok 1: ≈Åadowanie przedtrenowanego modelu
# Model Inception V3 zosta≈Ç wytrenowany na 1000 kategorii obiekt√≥w
def load_model(model_path):
    """
    ≈Åaduje zamro≈ºony graf TensorFlow z modelem.
    
    Args:
        model_path: ≈õcie≈ºka do pliku .pb z modelem
    
    Returns:
        Za≈Çadowany graf TensorFlow
    """
    graph = tf.Graph()
    graph_def = tf.compat.v1.GraphDef()
    
    with open(model_path, "rb") as f:
        graph_def.ParseFromString(f.read())
    
    with graph.as_default():
        tf.import_graph_def(graph_def)
    
    return graph

# Krok 2: Przetwarzanie obrazu z kamery robota
def preprocess_image(image_path, target_size=(299, 299)):
    """
    Przygotowuje obraz do analizy przez sieƒá neuronowƒÖ.
    
    Dlaczego te kroki:
    - Zmiana rozmiaru: Model wymaga obraz√≥w 299x299 pikseli
    - Normalizacja: Warto≈õci pikseli muszƒÖ byƒá w zakresie [0, 1]
    
    Args:
        image_path: ≈õcie≈ºka do obrazu z kamery
        target_size: rozmiar wymagany przez model
    
    Returns:
        Przetworzony obraz gotowy do analizy
    """
    # Wczytanie obrazu
    img = Image.open(image_path)
    
    # Zmiana rozmiaru do wymaganego przez model
    img = img.resize(target_size)
    
    # Konwersja do tablicy numpy
    img_array = np.array(img)
    
    # Dodanie wymiaru batch (model oczekuje wielu obraz√≥w naraz)
    img_array = np.expand_dims(img_array, axis=0)
    
    # Normalizacja warto≈õci pikseli do zakresu [0, 1]
    img_array = img_array.astype('float32') / 255.0
    
    return img_array

# Krok 3: Rozpoznawanie obiekt√≥w
def detect_objects(graph, image_array, labels_path):
    """
    Uruchamia model na obrazie i zwraca wyniki.
    
    Args:
        graph: za≈Çadowany model TensorFlow
        image_array: przetworzony obraz
        labels_path: ≈õcie≈ºka do pliku z etykietami
    
    Returns:
        Lista rozpoznanych obiekt√≥w z prawdopodobie≈Ñstwem
    """
    # Wczytanie etykiet (nazw kategorii)
    with open(labels_path, 'r') as f:
        labels = [line.strip() for line in f.readlines()]
    
    # Uruchomienie modelu
    with tf.compat.v1.Session(graph=graph) as sess:
        # Pobranie warstw wej≈õciowej i wyj≈õciowej
        input_tensor = graph.get_tensor_by_name('import/input:0')
        output_tensor = graph.get_tensor_by_name('import/InceptionV3/Predictions/Reshape_1:0')
        
        # Wykonanie predykcji
        predictions = sess.run(output_tensor, {input_tensor: image_array})
    
    # Przetworzenie wynik√≥w
    predictions = np.squeeze(predictions)  # Usuniƒôcie niepotrzebnych wymiar√≥w
    
    # Pobranie top 5 najbardziej prawdopodobnych obiekt√≥w
    top_5_indices = predictions.argsort()[-5:][::-1]
    
    results = []
    for i in top_5_indices:
        results.append({
            'obiekt': labels[i],
            'pewno≈õƒá': float(predictions[i])
        })
    
    return results

# Przyk≈Çad u≈ºycia dla robota G1
if __name__ == "__main__":
    # ≈öcie≈ºki do modelu i etykiet
    MODEL_PATH = "models/inception_v3_frozen.pb"
    LABELS_PATH = "models/imagenet_labels.txt"
    
    # Za≈Çaduj model (raz na poczƒÖtku)
    print("≈Åadowanie modelu...")
    model = load_model(MODEL_PATH)
    print("Model za≈Çadowany!")
    
    # Symulacja obrazu z kamery robota
    # W prawdziwej aplikacji pobierasz obraz bezpo≈õrednio z kamery G1
    image_path = "camera_capture.jpg"
    
    # Przetw√≥rz obraz
    processed_image = preprocess_image(image_path)
    
    # Rozpoznaj obiekty
    results = detect_objects(model, processed_image, LABELS_PATH)
    
    # Wy≈õwietl wyniki
    print("\nRozpoznane obiekty:")
    for result in results:
        print(f"  {result['obiekt']}: {result['pewno≈õƒá']*100:.2f}%")
    
    # Robot mo≈ºe teraz zareagowaƒá na wykryty obiekt
    # np. je≈õli wykryto "fili≈ºankƒô", robot mo≈ºe jƒÖ podnie≈õƒá
```

### 2. Rozpoznawanie polece≈Ñ g≈Çosowych (Speech Recognition)

**Cel:** Robot reaguje na polecenia g≈Çosowe u≈ºytkownika.

**Przypadki u≈ºycia:**
- Sterowanie g≈Çosowe ("id≈∫", "stop", "podnie≈õ")
- Interakcja cz≈Çowiek-robot
- Nawigacja na polecenia

**Implementacja:**

```python
# voice_commands_for_g1.py
"""
System rozpoznawania polece≈Ñ g≈Çosowych dla robota Unitree G1
Oparty na przyk≈Çadzie speech_commands z TensorFlow
"""

import tensorflow as tf
import numpy as np
import pyaudio
import wave

# Konfiguracja audio
SAMPLE_RATE = 16000  # Czƒôstotliwo≈õƒá pr√≥bkowania (Hz)
DURATION = 1         # Czas nagrywania (sekundy)

# Polecenia, kt√≥re robot rozumie
COMMANDS = [
    'idz',           # Go / Move forward
    'stop',          # Stop
    'lewo',          # Left
    'prawo',         # Right
    'podnies',       # Pick up
    'poloz',         # Put down
    'tak',           # Yes
    'nie'            # No
]

class VoiceCommandRecognizer:
    """
    Klasa do rozpoznawania polece≈Ñ g≈Çosowych dla robota.
    
    U≈ºywa modelu wytrenowanego na zestawie danych speech_commands.
    """
    
    def __init__(self, model_path):
        """
        Inicjalizacja rozpoznawania g≈Çosu.
        
        Args:
            model_path: ≈õcie≈ºka do wytrenowanego modelu
        """
        print("≈Åadowanie modelu rozpoznawania g≈Çosu...")
        self.model = tf.keras.models.load_model(model_path)
        print("Model za≈Çadowany!")
        
        # Inicjalizacja PyAudio do nagrywania
        self.audio = pyaudio.PyAudio()
    
    def record_audio(self):
        """
        Nagrywa kr√≥tki fragment audio z mikrofonu robota.
        
        Returns:
            Tablica numpy z danymi audio
        """
        # Otw√≥rz strumie≈Ñ audio
        stream = self.audio.open(
            format=pyaudio.paInt16,      # 16-bit audio
            channels=1,                   # Mono
            rate=SAMPLE_RATE,
            input=True,
            frames_per_buffer=1024
        )
        
        print("Nagrywam polecenie...")
        frames = []
        
        # Nagraj DURATION sekund audio
        for i in range(0, int(SAMPLE_RATE / 1024 * DURATION)):
            data = stream.read(1024)
            frames.append(data)
        
        print("Nagrywanie zako≈Ñczone.")
        
        # Zamknij strumie≈Ñ
        stream.stop_stream()
        stream.close()
        
        # Konwertuj do tablicy numpy
        audio_data = np.frombuffer(b''.join(frames), dtype=np.int16)
        
        return audio_data
    
    def preprocess_audio(self, audio_data):
        """
        Przetwarza dane audio do formatu wymaganego przez model.
        
        Kroki przetwarzania:
        1. Normalizacja - Warto≈õci w zakresie [-1, 1]
        2. Spektrogram - Przekszta≈Çcenie do reprezentacji czƒôstotliwo≈õciowej
        3. MFCC - Mel-Frequency Cepstral Coefficients (standardowa reprezentacja dla mowy)
        
        Args:
            audio_data: Surowe dane audio
        
        Returns:
            Przetworzony tensor gotowy do modelu
        """
        # Normalizacja
        audio_normalized = audio_data.astype(np.float32) / 32768.0
        
        # Konwersja do tensora TensorFlow
        audio_tensor = tf.constant(audio_normalized)
        
        # Tutaj normalnie obliczasz spektrogram lub MFCC
        # Dla uproszczenia u≈ºywamy surowego audio
        # W prawdziwej implementacji u≈ºyj tf.signal.stft i tf.signal.mfccs_from_log_mel_spectrogram
        
        # Dodaj wymiar batch
        audio_tensor = tf.expand_dims(audio_tensor, 0)
        
        return audio_tensor
    
    def recognize_command(self):
        """
        Nagrywa audio i rozpoznaje polecenie.
        
        Returns:
            Rozpoznane polecenie jako string
        """
        # Nagraj audio z mikrofonu
        audio_data = self.record_audio()
        
        # Przetw√≥rz audio
        processed_audio = self.preprocess_audio(audio_data)
        
        # Uruchom model
        predictions = self.model.predict(processed_audio)
        
        # Znajd≈∫ najbardziej prawdopodobne polecenie
        command_index = np.argmax(predictions[0])
        confidence = predictions[0][command_index]
        
        command = COMMANDS[command_index]
        
        print(f"Rozpoznano: {command} (pewno≈õƒá: {confidence*100:.1f}%)")
        
        return command, confidence
    
    def cleanup(self):
        """Zamkniƒôcie zasob√≥w audio."""
        self.audio.terminate()

# Przyk≈Çad u≈ºycia dla robota G1
def main():
    """
    G≈Ç√≥wna pƒôtla rozpoznawania polece≈Ñ g≈Çosowych.
    Robot nas≈Çuchuje polece≈Ñ i reaguje na nie.
    """
    # Inicjalizacja rozpoznawania g≈Çosu
    recognizer = VoiceCommandRecognizer('models/speech_commands_model.h5')
    
    print("\n" + "="*50)
    print("Robot Unitree G1 - System rozpoznawania g≈Çosu")
    print("="*50)
    print("\nDostƒôpne polecenia:")
    for cmd in COMMANDS:
        print(f"  - {cmd}")
    print("\nNaci≈õnij Ctrl+C aby zako≈Ñczyƒá.\n")
    
    try:
        while True:
            # Nas≈Çuchuj polecenia
            command, confidence = recognizer.recognize_command()
            
            # Reaguj tylko je≈õli pewno≈õƒá > 70%
            if confidence > 0.7:
                print(f"\n>>> Wykonujƒô polecenie: {command}")
                
                # Tutaj dodaj kod sterujƒÖcy robotem
                if command == 'idz':
                    print("Robot porusza siƒô do przodu...")
                    # robot.move_forward()
                
                elif command == 'stop':
                    print("Robot zatrzymuje siƒô...")
                    # robot.stop()
                
                elif command == 'lewo':
                    print("Robot obraca siƒô w lewo...")
                    # robot.turn_left()
                
                elif command == 'prawo':
                    print("Robot obraca siƒô w prawo...")
                    # robot.turn_right()
                
                elif command == 'podnies':
                    print("Robot podnosi obiekt...")
                    # robot.pick_up()
                
                elif command == 'poloz':
                    print("Robot k≈Çadzie obiekt...")
                    # robot.put_down()
                
                print()
            else:
                print(f"Polecenie niezrozumia≈Çe (pewno≈õƒá tylko {confidence*100:.1f}%)")
            
            # Kr√≥tka pauza przed nastƒôpnym nagraniem
            import time
            time.sleep(0.5)
    
    except KeyboardInterrupt:
        print("\n\nZamykanie systemu...")
        recognizer.cleanup()
        print("Do widzenia!")

if __name__ == "__main__":
    main()
```

### 3. Nawigacja i unikanie przeszk√≥d

**Cel:** Robot nawiguje autonomicznie, unikajƒÖc przeszk√≥d.

**Przypadki u≈ºycia:**
- Poruszanie siƒô po pomieszczeniu
- Mapowanie otoczenia
- Planowanie ≈õcie≈ºki

**Kluczowe komponenty:**
- **Detekcja przeszk√≥d** - Wykorzystanie kamer i modelu segmentacji obrazu
- **Mapowanie** - SLAM (Simultaneous Localization and Mapping)
- **Planowanie ≈õcie≈ºki** - Algorytmy pathfinding z uczeniem maszynowym

## Przyk≈Çadowe projekty

### Projekt 1: Asystent do rozpoznawania obiekt√≥w

**Zadanie:** Robot rozpoznaje przedmioty na stole i informuje u≈ºytkownika.

**Kroki:**
1. Konfiguracja kamery robota
2. Za≈Çadowanie modelu Inception V3
3. CiƒÖg≈Çe przetwarzanie obrazu z kamery
4. Wy≈õwietlanie rozpoznanych obiekt√≥w

### Projekt 2: Sterowanie g≈Çosowe podstawowymi ruchami

**Zadanie:** Robot reaguje na polecenia g≈Çosowe (id≈∫, stop, lewo, prawo).

**Kroki:**
1. Trening modelu na polskich komendach
2. Integracja z mikrofonami robota
3. Implementacja logiki sterowania
4. Testowanie i dostrajanie

### Projekt 3: Autonomiczne dostarczanie obiekt√≥w

**Zadanie:** Robot odbiera przedmiot z punktu A i dostarcza do punktu B.

**Kroki:**
1. Rozpoznawanie obiektu docelowego
2. Planowanie ≈õcie≈ºki do obiektu
3. Chwytanie obiektu
4. Nawigacja do miejsca docelowego
5. Po≈Ço≈ºenie obiektu

## RozwiƒÖzywanie problem√≥w

### Problem: Model dzia≈Ça zbyt wolno

**Przyczyna:** NiewystarczajƒÖca moc obliczeniowa.

**RozwiƒÖzania:**
1. U≈ºyj TensorFlow Lite zamiast pe≈Çnego TensorFlow
2. Wykorzystaj GPU je≈õli dostƒôpne
3. Zmniejsz rozdzielczo≈õƒá wej≈õciowego obrazu
4. U≈ºyj mniejszego modelu (np. MobileNet zamiast Inception)

### Problem: Niskie accuracy rozpoznawania

**Przyczyna:** Model nie pasuje do konkretnego przypadku u≈ºycia.

**RozwiƒÖzania:**
1. Dostrajanie (fine-tuning) modelu na w≈Çasnych danych
2. Zbieranie wiƒôkszej ilo≈õci danych treningowych
3. Augmentacja danych (obroty, przesuniƒôcia, etc.)
4. Wyb√≥r lepszego modelu dla konkretnego zadania

### Problem: Rozpoznawanie g≈Çosu nie dzia≈Ça

**Przyczyna:** Zak≈Ç√≥cenia, nieprawid≈Çowa konfiguracja mikrofonu.

**RozwiƒÖzania:**
1. Sprawd≈∫ konfiguracjƒô mikrofonu (sample rate, formaty)
2. Dodaj filtrowanie szum√≥w
3. Zwiƒôksz czas nagrywania pr√≥bki
4. Trenuj model na danych z szumem t≈Ça

## Podsumowanie

TensorFlow to potƒô≈ºne narzƒôdzie, kt√≥re otwiera przed robotem Unitree G1 EDU-U6 nieograniczone mo≈ºliwo≈õci:
- **Percepcja** - Widzenie i s≈Çyszenie
- **Inteligencja** - Rozumienie i uczenie siƒô
- **Autonomia** - Samodzielne podejmowanie decyzji

**Nastƒôpne kroki:**
1. Przejd≈∫ przez przyk≈Çadowy kod w `tensorflow/examples/`
2. Eksperymentuj z w≈Çasnymi danymi
3. Buduj w≈Çasne modele dla specyficznych zada≈Ñ
4. Dziel siƒô swoimi projektami ze spo≈Çeczno≈õciƒÖ!

---

**Pytania? Problemy?**
- Sprawd≈∫ [README_PL.md](README_PL.md) dla og√≥lnych informacji
- Zobacz przyk≈Çady kodu w `tensorflow/examples/`
- Odwied≈∫ [forum TensorFlow](https://discuss.tensorflow.org/)

**Powodzenia w projektach z robotem Unitree G1!** ü§ñ‚ú®

# Rozpoznawanie Polece≈Ñ G≈Çosowych - Przewodnik dla Student√≥w

## Wprowadzenie

Ten przyk≈Çad pokazuje jak zbudowaƒá system rozpoznawania mowy wykorzystujƒÖcy TensorFlow. Jest to fundamentalna technologia dla robota Unitree G1 EDU-U6, pozwalajƒÖca mu reagowaƒá na polecenia g≈Çosowe u≈ºytkownika.

## Co robi ten system?

System rozpoznaje **kr√≥tkie s≈Çowa-komendy** z ograniczonego s≈Çownika. Standardowo rozpoznaje s≈Çowa:
- **"yes"** (tak) / **"no"** (nie)
- **"up"** (g√≥ra) / **"down"** (d√≥≈Ç)
- **"left"** (lewo) / **"right"** (prawo)
- **"on"** (w≈ÇƒÖcz) / **"off"** (wy≈ÇƒÖcz)
- **"stop"** (stop) / **"go"** (id≈∫)

### To NIE jest pe≈Çny system rozpoznawania mowy

**Co potrafi:**
- ‚úÖ Rozpoznaje pojedyncze, kr√≥tkie s≈Çowa
- ‚úÖ Dzia≈Ça w czasie rzeczywistym
- ‚úÖ Mo≈ºna trenowaƒá na w≈Çasnych s≈Çowach (r√≥wnie≈º polskich!)
- ‚úÖ Odpowiedni dla robotyki i urzƒÖdze≈Ñ embedded

**Czego nie potrafi:**
- ‚ùå Rozpoznawanie pe≈Çnych zda≈Ñ
- ‚ùå Transkrypcja d≈Çugich wypowiedzi
- ‚ùå Rozpoznawanie mowy ciƒÖg≈Çej

**Dla zaawansowanego rozpoznawania mowy** polecamy systemy takie jak Kaldi, Whisper lub Google Cloud Speech-to-Text.

## Zastosowania w robocie Unitree G1 EDU-U6

### Przyk≈Çadowe scenariusze

**Scenariusz 1: Sterowanie ruchem robota**
```
U≈ºytkownik: "id≈∫"     ‚Üí Robot porusza siƒô do przodu
U≈ºytkownik: "stop"    ‚Üí Robot zatrzymuje siƒô
U≈ºytkownik: "lewo"    ‚Üí Robot obraca siƒô w lewo
U≈ºytkownik: "prawo"   ‚Üí Robot obraca siƒô w prawo
```

**Scenariusz 2: Kontrola manipulatora**
```
U≈ºytkownik: "podnie≈õ" ‚Üí Robot podnosi przedmiot
U≈ºytkownik: "po≈Ç√≥≈º"   ‚Üí Robot k≈Çadzie przedmiot
U≈ºytkownik: "otw√≥rz"  ‚Üí Robot otwiera chwyt
U≈ºytkownik: "zamknij" ‚Üí Robot zamyka chwyt
```

**Scenariusz 3: Interakcja z u≈ºytkownikiem**
```
U≈ºytkownik: "tak"     ‚Üí Robot potwierdza akcjƒô
U≈ºytkownik: "nie"     ‚Üí Robot anuluje akcjƒô
U≈ºytkownik: "pomoc"   ‚Üí Robot wy≈õwietla dostƒôpne komendy
```

## Struktura projektu

### G≈Ç√≥wne pliki

```
speech_commands/
‚îú‚îÄ‚îÄ train.py                    # Trenowanie modelu
‚îú‚îÄ‚îÄ freeze.py                   # Tworzenie wersji produkcyjnej modelu
‚îú‚îÄ‚îÄ input_data.py              # ≈Åadowanie i przetwarzanie danych audio
‚îú‚îÄ‚îÄ models.py                  # Definicje architektur sieci neuronowych
‚îú‚îÄ‚îÄ label_wav.py               # Rozpoznawanie pojedynczego pliku audio
‚îú‚îÄ‚îÄ label_wav_dir.py           # Rozpoznawanie ca≈Çego katalogu
‚îú‚îÄ‚îÄ recognize_commands.py      # Rozpoznawanie w czasie rzeczywistym
‚îî‚îÄ‚îÄ README_PL.md               # Ten plik
```

### Przep≈Çyw pracy

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  1. Zbierz dane     ‚îÇ  Nagraj pliki .wav z komendami
‚îÇ     audio           ‚îÇ  lub pobierz gotowy dataset
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
           ‚îÇ
           ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  2. Trenuj model    ‚îÇ  python train.py --wanted_words=id≈∫,stop,lewo,prawo
‚îÇ     (train.py)      ‚îÇ  
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
           ‚îÇ
           ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  3. Testuj model    ‚îÇ  python label_wav.py --wav=test.wav
‚îÇ     (label_wav.py)  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
           ‚îÇ
           ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  4. Wdr√≥≈º na        ‚îÇ  U≈ºyj zamro≈ºonego modelu w robocie
‚îÇ     robocie         ‚îÇ  dla rozpoznawania w czasie rzeczywistym
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

## Jak rozpoczƒÖƒá - Krok po kroku

### Krok 1: Przygotowanie ≈õrodowiska

```bash
# Upewnij siƒô ≈ºe masz zainstalowany TensorFlow
pip install tensorflow numpy

# Przejd≈∫ do katalogu speech_commands
cd tensorflow/examples/speech_commands
```

### Krok 2: Trening z gotowym zestawem danych

U≈ºyj domy≈õlnego datasetu Speech Commands (automatycznie siƒô pobierze):

```bash
# Podstawowy trening (u≈ºywa domy≈õlnych s≈Ç√≥w)
python train.py

# Trening z wybranymi s≈Çowami
python train.py --wanted_words=yes,no,up,down,left,right
```

**Co siƒô dzieje podczas treningu:**
1. **Pobieranie danych** (pierwszym razem)
   - Dataset Speech Commands (~1GB)
   - TysiƒÖce nagra≈Ñ .wav po 1 sekundzie ka≈ºde
   - R√≥≈ºni m√≥wcy, r√≥≈ºne warunki nagrania

2. **Preprocessing audio**
   - Konwersja audio do spektrogram√≥w
   - Augmentacja danych (szum, przesuniƒôcia czasowe)
   - Normalizacja

3. **Trening sieci neuronowej**
   - Kilka tysiƒôcy iteracji
   - Na CPU: 2-4 godziny
   - Na GPU: 30-60 minut

4. **Walidacja**
   - Sprawdzanie accuracy na zbiorze walidacyjnym
   - Powinno osiƒÖgnƒÖƒá >90% accuracy

**Przyk≈Çadowy output:**
```
INFO:tensorflow:Step #100: rate 0.001000, accuracy = 12.0%, cross entropy = 2.589
INFO:tensorflow:Step #200: rate 0.001000, accuracy = 25.0%, cross entropy = 2.234
INFO:tensorflow:Step #500: rate 0.001000, accuracy = 47.0%, cross entropy = 1.876
...
INFO:tensorflow:Step #4000: rate 0.001000, accuracy = 91.0%, cross entropy = 0.334
```

### Krok 3: Trening na w≈Çasnych polskich komendach

#### 3a. Przygotowanie danych

Utw√≥rz strukturƒô katalog√≥w:

```
moje_polskie_komendy/
‚îú‚îÄ‚îÄ idz/              # Polecenie "id≈∫"
‚îÇ   ‚îú‚îÄ‚îÄ nagranie001.wav
‚îÇ   ‚îú‚îÄ‚îÄ nagranie002.wav
‚îÇ   ‚îî‚îÄ‚îÄ ... (minimum 100 nagra≈Ñ)
‚îú‚îÄ‚îÄ stop/             # Polecenie "stop"
‚îÇ   ‚îú‚îÄ‚îÄ nagranie101.wav
‚îÇ   ‚îú‚îÄ‚îÄ nagranie102.wav
‚îÇ   ‚îî‚îÄ‚îÄ ...
‚îú‚îÄ‚îÄ lewo/             # Polecenie "lewo"
‚îÇ   ‚îî‚îÄ‚îÄ ...
‚îú‚îÄ‚îÄ prawo/            # Polecenie "prawo"
‚îÇ   ‚îî‚îÄ‚îÄ ...
‚îî‚îÄ‚îÄ inne/             # Inne d≈∫wiƒôki (szum, inne s≈Çowa)
    ‚îî‚îÄ‚îÄ ...
```

**Wymagania dla nagra≈Ñ:**
- **Format:** WAV, 16-bit PCM
- **Czƒôstotliwo≈õƒá:** 16000 Hz (16 kHz)
- **D≈Çugo≈õƒá:** 1 sekunda
- **Kana≈Çy:** Mono (1 kana≈Ç)
- **Jako≈õƒá:** Wyra≈∫na wymowa, ma≈Ço szumu t≈Ça

**Narzƒôdzie do nagrywania:**

```bash
# Linux/Mac - nagrywanie z mikrofonu
arecord -f cd -d 1 -r 16000 nagranie.wav

# Lub u≈ºyj Audacity (darmowy edytor audio)
# 1. Nagraj s≈Çowo
# 2. Eksportuj jako WAV 16kHz mono
```

**Wskaz√≥wki dla lepszych wynik√≥w:**
- Nagraj minimum 100 przyk≈Çad√≥w ka≈ºdego s≈Çowa
- U≈ºyj r√≥≈ºnych m√≥wc√≥w (mƒô≈ºczy≈∫ni, kobiety, r√≥≈ºne akcenty)
- Nagraj w r√≥≈ºnych warunkach (ciche pomieszczenie, z lekkim szumem)
- S≈Çowo powinno byƒá wypowiadane w ≈õrodku 1-sekundowego nagrania

#### 3b. Uruchomienie treningu

```bash
python train.py \
  --data_dir=moje_polskie_komendy \
  --wanted_words=idz,stop,lewo,prawo \
  --train_dir=/tmp/polski_speech_model \
  --how_many_training_steps=4000,1000 \
  --learning_rate=0.001,0.0001
```

**Parametry wyja≈õnione:**
- `--data_dir`: Katalog z Twoimi nagraniami
- `--wanted_words`: S≈Çowa kt√≥re chcesz rozpoznawaƒá (oddzielone przecinkami)
- `--train_dir`: Gdzie zapisaƒá wytrenowany model
- `--how_many_training_steps`: Liczba krok√≥w treningu (wiƒôcej = lepszy model, ale d≈Çu≈ºej)
- `--learning_rate`: Szybko≈õƒá uczenia (zaczyna od 0.001, potem spada do 0.0001)

### Krok 4: Testowanie wytrenowanego modelu

#### Test pojedynczego pliku

```bash
python label_wav.py \
  --wav=test_nagranie.wav \
  --graph=/tmp/polski_speech_model/frozen_graph.pb \
  --labels=/tmp/polski_speech_model/labels.txt
```

**Przyk≈Çadowy wynik:**
```
≈Åadowanie modelu...
Przetwarzanie: test_nagranie.wav

Wyniki rozpoznawania:
  idz:    0.89  (89%)
  stop:   0.05  (5%)
  lewo:   0.03  (3%)
  prawo:  0.02  (2%)

Rozpoznane polecenie: idz (pewno≈õƒá: 89%)
```

#### Test ca≈Çego katalogu

```bash
python label_wav_dir.py \
  --wav_dir=testy/ \
  --graph=/tmp/polski_speech_model/frozen_graph.pb \
  --labels=/tmp/polski_speech_model/labels.txt
```

### Krok 5: Zamro≈ºenie modelu dla produkcji

Po zako≈Ñczeniu treningu, utw√≥rz zoptymalizowanƒÖ wersjƒô modelu:

```bash
python freeze.py \
  --start_checkpoint=/tmp/polski_speech_model/conv.ckpt-4000 \
  --output_file=/tmp/frozen_graph.pb
```

**Dlaczego zamra≈ºamy model:**
- ≈ÅƒÖczy wagi i architekturƒô w jeden plik
- Optymalizuje dla inferencji (nie dla treningu)
- Zmniejsza rozmiar
- ≈Åatwiejszy do wdro≈ºenia

## Jak dzia≈Ça rozpoznawanie mowy - Teoria

### 1. Przetwarzanie sygna≈Çu audio

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Fala        ‚îÇ    ‚îÇ  Spectro-    ‚îÇ    ‚îÇ  MFCC        ‚îÇ
‚îÇ  d≈∫wiƒôkowa   ‚îÇ -> ‚îÇ  gram        ‚îÇ -> ‚îÇ  (cechy)     ‚îÇ
‚îÇ  (surowe)    ‚îÇ    ‚îÇ              ‚îÇ    ‚îÇ              ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**Fala d≈∫wiƒôkowa:**
- Surowy sygna≈Ç z mikrofonu
- Warto≈õci amplitudy w czasie
- Trudny do analizy bezpo≈õrednio

**Spektrogram:**
- Reprezentacja czƒôstotliwo≈õci w czasie
- Transformata Fouriera (STFT)
- Pokazuje jakie czƒôstotliwo≈õci wystƒôpujƒÖ w danym momencie

**MFCC (Mel-Frequency Cepstral Coefficients):**
- Kompresja informacji ze spektrogramu
- Inspirowane ludzkim s≈Çuchem
- Standardowa reprezentacja dla mowy

### 2. Sieƒá neuronowa

Model u≈ºywa konwolucyjnej sieci neuronowej (CNN):

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Wej≈õcie    ‚îÇ  Spektrogram 49x40
‚îÇ  (MFCC)     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
       ‚îÇ
       ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Conv2D     ‚îÇ  Wykrywanie podstawowych wzorc√≥w
‚îÇ  + ReLU     ‚îÇ  (krawƒôdzie, prƒÖ≈ºki w spektrogramie)
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
       ‚îÇ
       ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Pooling    ‚îÇ  Redukcja rozmiaru
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
       ‚îÇ
       ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Conv2D     ‚îÇ  Wykrywanie bardziej z≈Ço≈ºonych wzorc√≥w
‚îÇ  + ReLU     ‚îÇ  (fonemy, sylaby)
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
       ‚îÇ
       ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Pooling    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
       ‚îÇ
       ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Flatten    ‚îÇ  Sp≈Çaszczenie do wektora
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
       ‚îÇ
       ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Dense      ‚îÇ  Klasyfikacja
‚îÇ  (Softmax)  ‚îÇ  Prawdopodobie≈Ñstwo dla ka≈ºdego s≈Çowa
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
       ‚îÇ
       ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Wynik      ‚îÇ  [0.89, 0.05, 0.03, 0.02]
‚îÇ             ‚îÇ  "idz" z 89% pewno≈õciƒÖ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### 3. Rozpoznawanie w czasie rzeczywistym

Dla aplikacji robotycznych potrzebujemy rozpoznawania na bie≈ºƒÖco:

```python
from recognize_commands import RecognizeCommands

# Inicjalizacja
recognizer = RecognizeCommands(
    labels=['idz', 'stop', 'lewo', 'prawo'],
    average_window_duration_ms=1000,
    detection_threshold=0.7
)

# Pƒôtla rozpoznawania
while robot.is_active():
    # Nagraj kr√≥tki fragment audio
    audio_data = microphone.record(duration_ms=100)
    
    # Przetw√≥rz audio
    spectrogram = compute_spectrogram(audio_data)
    
    # Uruchom model
    predictions = model.predict(spectrogram)
    
    # Sprawd≈∫ czy wykryto komendƒô
    command = recognizer.process(predictions, current_time_ms)
    
    if command:
        print(f"Wykryto komendƒô: {command}")
        robot.execute_command(command)
```

## Integracja z robotem Unitree G1

### Kompletny przyk≈Çad systemu sterowania g≈Çosem

```python
"""
System sterowania g≈Çosowego dla robota Unitree G1 EDU-U6
"""

import tensorflow as tf
import numpy as np
import pyaudio
import threading
from collections import deque

class VoiceControlledRobot:
    """Robot sterowany g≈Çosem u≈ºywajƒÖcy TensorFlow."""
    
    def __init__(self, model_path, labels_path):
        """
        Inicjalizacja systemu.
        
        Args:
            model_path: ≈öcie≈ºka do zamro≈ºonego modelu .pb
            labels_path: ≈öcie≈ºka do pliku z etykietami
        """
        # ≈Åadowanie modelu TensorFlow
        self.graph = self.load_frozen_graph(model_path)
        self.labels = self.load_labels(labels_path)
        
        # Konfiguracja audio
        self.SAMPLE_RATE = 16000
        self.CHUNK_DURATION_MS = 100  # 100ms na chunk
        self.CHUNK_SIZE = int(self.SAMPLE_RATE * self.CHUNK_DURATION_MS / 1000)
        
        # Buffer dla okna czasowego
        self.audio_buffer = deque(maxlen=16)  # 1.6s bufora
        
        # PyAudio do nagrywania
        self.audio = pyaudio.PyAudio()
        self.stream = None
        
        # Stan rozpoznawania
        self.is_listening = False
        self.last_command = None
        self.command_callback = None
        
    def load_frozen_graph(self, model_path):
        """≈Åaduje zamro≈ºony model TensorFlow."""
        graph = tf.Graph()
        with tf.io.gfile.GFile(model_path, 'rb') as f:
            graph_def = tf.compat.v1.GraphDef()
            graph_def.ParseFromString(f.read())
        
        with graph.as_default():
            tf.import_graph_def(graph_def)
        
        return graph
    
    def load_labels(self, labels_path):
        """Wczytuje listƒô etykiet."""
        with open(labels_path, 'r') as f:
            return [line.strip() for line in f.readlines()]
    
    def start_listening(self, callback):
        """
        Rozpoczyna nas≈Çuchiwanie polece≈Ñ g≈Çosowych.
        
        Args:
            callback: Funkcja wywo≈Çywana gdy rozpoznano komendƒô
                     callback(command_name, confidence)
        """
        self.command_callback = callback
        self.is_listening = True
        
        # Otw√≥rz strumie≈Ñ audio
        self.stream = self.audio.open(
            format=pyaudio.paInt16,
            channels=1,
            rate=self.SAMPLE_RATE,
            input=True,
            frames_per_buffer=self.CHUNK_SIZE,
            stream_callback=self._audio_callback
        )
        
        self.stream.start_stream()
        print("üé§ Nas≈Çuchujƒô polece≈Ñ g≈Çosowych...")
    
    def _audio_callback(self, in_data, frame_count, time_info, status):
        """Callback wywo≈Çywany dla ka≈ºdego chunka audio."""
        # Konwertuj bajty do numpy array
        audio_data = np.frombuffer(in_data, dtype=np.int16)
        
        # Dodaj do bufora
        self.audio_buffer.append(audio_data)
        
        # Gdy mamy wystarczajƒÖco danych, rozpoznaj
        if len(self.audio_buffer) == self.audio_buffer.maxlen:
            # Uruchom rozpoznawanie w osobnym wƒÖtku
            # (aby nie blokowaƒá strumienia audio)
            threading.Thread(
                target=self._recognize_from_buffer
            ).start()
        
        return (None, pyaudio.paContinue)
    
    def _recognize_from_buffer(self):
        """Rozpoznaje komendƒô z bufora audio."""
        # Po≈ÇƒÖcz chunks w jeden sygna≈Ç
        audio_data = np.concatenate(list(self.audio_buffer))
        
        # Przetw√≥rz audio do spektrogramu (uproszczone)
        # W prawdziwej implementacji u≈ºyj tf.signal lub librosa
        spectrogram = self._compute_spectrogram(audio_data)
        
        # Uruchom model
        with tf.compat.v1.Session(graph=self.graph) as sess:
            input_tensor = self.graph.get_tensor_by_name('import/input:0')
            output_tensor = self.graph.get_tensor_by_name('import/output:0')
            
            predictions = sess.run(output_tensor, {
                input_tensor: spectrogram
            })
        
        # Pobierz najlepszƒÖ predykcjƒô
        predictions = np.squeeze(predictions)
        top_index = np.argmax(predictions)
        confidence = predictions[top_index]
        
        # Je≈õli pewno≈õƒá wystarczajƒÖco wysoka
        if confidence > 0.7:
            command = self.labels[top_index]
            
            # Unikaj powtarzania tej samej komendy
            if command != self.last_command:
                self.last_command = command
                
                # Wywo≈Çaj callback
                if self.command_callback:
                    self.command_callback(command, confidence)
    
    def _compute_spectrogram(self, audio_data):
        """Oblicza spektrogram z sygna≈Çu audio."""
        # Tutaj powinno byƒá pe≈Çne przetwarzanie audio
        # Dla uproszczenia zwracamy placeholder
        # W prawdziwej implementacji u≈ºyj:
        # - tf.signal.stft dla Short-Time Fourier Transform
        # - tf.signal.mfccs_from_log_mel_spectrogram dla MFCC
        
        # Przyk≈Çad (uproszczony):
        stft = tf.signal.stft(
            tf.cast(audio_data, tf.float32),
            frame_length=480,
            frame_step=160
        )
        spectrogram = tf.abs(stft)
        
        # Reshape do oczekiwanego formatu
        spectrogram = tf.expand_dims(spectrogram, 0)
        spectrogram = tf.expand_dims(spectrogram, -1)
        
        return spectrogram.numpy()
    
    def stop_listening(self):
        """Zatrzymuje nas≈Çuchiwanie."""
        self.is_listening = False
        if self.stream:
            self.stream.stop_stream()
            self.stream.close()
        print("üîá Nas≈Çuchiwanie zatrzymane.")


# Przyk≈Çadowe u≈ºycie z robotem
def main():
    """G≈Ç√≥wna funkcja demonstracyjna."""
    
    # Inicjalizacja robota (przyk≈Çad)
    # robot = UnitreeG1Robot()
    
    # Inicjalizacja systemu rozpoznawania g≈Çosu
    voice_system = VoiceControlledRobot(
        model_path='models/frozen_graph.pb',
        labels_path='models/labels.txt'
    )
    
    # Funkcja obs≈ÇugujƒÖca komendy
    def handle_command(command, confidence):
        """Reaguje na rozpoznanƒÖ komendƒô."""
        print(f"\nü§ñ Komenda: {command} (pewno≈õƒá: {confidence*100:.1f}%)")
        
        # Symulacja reakcji robota
        if command == 'idz':
            print("   ‚Üí Robot idzie do przodu")
            # robot.move_forward()
        
        elif command == 'stop':
            print("   ‚Üí Robot zatrzymuje siƒô")
            # robot.stop()
        
        elif command == 'lewo':
            print("   ‚Üí Robot obraca w lewo")
            # robot.turn_left()
        
        elif command == 'prawo':
            print("   ‚Üí Robot obraca w prawo")
            # robot.turn_right()
        
        elif command == 'podnies':
            print("   ‚Üí Robot podnosi obiekt")
            # robot.pick_up()
        
        elif command == 'poloz':
            print("   ‚Üí Robot k≈Çadzie obiekt")
            # robot.put_down()
    
    # Rozpocznij nas≈Çuchiwanie
    print("="*60)
    print(" System sterowania g≈Çosowego - Unitree G1")
    print("="*60)
    voice_system.start_listening(handle_command)
    
    try:
        # Czekaj na Ctrl+C
        while True:
            import time
            time.sleep(0.1)
    except KeyboardInterrupt:
        print("\n\nZatrzymywanie...")
        voice_system.stop_listening()
        print("Do widzenia!")


if __name__ == '__main__':
    main()
```

## Optymalizacja dla robota

### 1. U≈ºycie TensorFlow Lite

Dla lepszej wydajno≈õci na robocie:

```bash
# Konwersja do TensorFlow Lite
python -c "
import tensorflow as tf

converter = tf.lite.TFLiteConverter.from_frozen_graph(
    'frozen_graph.pb',
    input_arrays=['input'],
    output_arrays=['output']
)

# Optymalizacja
converter.optimizations = [tf.lite.Optimize.DEFAULT]

# Kwantyzacja (int8) dla jeszcze lepszej wydajno≈õci
tflite_model = converter.convert()

with open('model.tflite', 'wb') as f:
    f.write(tflite_model)
"
```

### 2. Parametry optymalizacji

```python
# Kompromis miƒôdzy latencjƒÖ a accuracy
OPTIMIZATION_PARAMS = {
    'window_size_ms': 30,        # Mniejsze okno = szybsza reakcja
    'window_stride_ms': 10,      # Mniejszy stride = lepsza accuracy
    'feature_bin_count': 32,     # Mniej cech = szybsze przetwarzanie
    'detection_threshold': 0.75  # Wy≈ºszy pr√≥g = mniej fa≈Çszywych wykryƒá
}
```

## RozwiƒÖzywanie problem√≥w

### Problem: Niska accuracy rozpoznawania

**Przyczyny i rozwiƒÖzania:**

1. **Za ma≈Ço danych treningowych**
   - RozwiƒÖzanie: Nagraj wiƒôcej przyk≈Çad√≥w (min. 100 na s≈Çowo)

2. **Szum t≈Ça**
   - RozwiƒÖzanie: Dodaj przyk≈Çady z szumem do treningu
   - RozwiƒÖzanie: U≈ºyj filtrowania szumu (np. Wiener filter)

3. **R√≥≈ºni m√≥wcy**
   - RozwiƒÖzanie: Trenuj na nagraniach r√≥≈ºnych os√≥b
   - RozwiƒÖzanie: Augmentacja danych (zmiana pitch, tempo)

### Problem: Fa≈Çszywe wykrycia

**RozwiƒÖzanie:**
```python
# Zwiƒôksz pr√≥g pewno≈õci
detection_threshold = 0.8  # Domy≈õlnie 0.7

# U≈ºyj u≈õredniania w czasie
average_window_duration_ms = 1500  # Domy≈õlnie 1000

# Dodaj mechanizm potwierdzania
def confirm_command(command, history, min_repeats=2):
    """Potwierdza komendƒô tylko gdy pojawi≈Ça siƒô wielokrotnie."""
    recent = history[-5:]
    count = recent.count(command)
    return count >= min_repeats
```

### Problem: Op√≥≈∫nienie w rozpoznawaniu

**RozwiƒÖzanie:**
```python
# Zmniejsz rozmiar okna
window_size_ms = 20  # Domy≈õlnie 30

# U≈ºyj mniejszego modelu
model_architecture = 'tiny_conv'  # Zamiast 'conv'

# U≈ºyj TensorFlow Lite
# (3-5x szybsze ni≈º pe≈Çny TensorFlow)
```

## Dodatkowe zasoby

### Dokumentacja
- [Oficjalny tutorial TensorFlow Audio](https://www.tensorflow.org/tutorials/audio/simple_audio)
- [Speech Commands Dataset](https://blog.research.google/2017/08/launching-speech-commands-dataset.html)

### Narzƒôdzia
- [Audacity](https://www.audacityteam.org/) - Darmowy edytor audio
- [SoX](http://sox.sourceforge.net/) - Przetwarzanie audio z linii polece≈Ñ

### Zaawansowane systemy (dla ambitnych)
- [Kaldi](https://kaldi-asr.org/) - Profesjonalny system rozpoznawania mowy
- [Mozilla DeepSpeech](https://github.com/mozilla/DeepSpeech) - Open source speech-to-text
- [Whisper](https://github.com/openai/whisper) - Model od OpenAI

## Podsumowanie

System rozpoznawania polece≈Ñ g≈Çosowych oferuje:
- ‚úÖ Prosty interfejs g≈Çosowy dla robota
- ‚úÖ Mo≈ºliwo≈õƒá trenowania na w≈Çasnych s≈Çowach (w tym polskich)
- ‚úÖ Dzia≈Çanie w czasie rzeczywistym
- ‚úÖ Odpowiednia wydajno≈õƒá dla robotyki

**Nastƒôpne kroki:**
1. Przejd≈∫ przez przyk≈Çad treningu
2. Nagraj w≈Çasne polskie komendy
3. Wytrenuj model i przetestuj
4. Zintegruj z robotem Unitree G1
5. Eksperymentuj i udoskonalaj!

**Powodzenia w tworzeniu robota sterowanego g≈Çosem! ü§ñüé§**

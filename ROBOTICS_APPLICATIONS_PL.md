# TensorFlow w Robotyce - Podsumowanie i Zastosowania
## Dokument dla projektu z robotem Unitree G1 EDU-U6

### Politechnika Rzeszowska - Laboratorium AI i Robotyki

---

## 1. Wprowadzenie

### 1.1 Czym jest TensorFlow?

TensorFlow to otwarta platforma do uczenia maszynowego (machine learning) stworzona przez Google. W kontekÅ›cie robotyki TensorFlow pozwala robotom:

- **"WidzieÄ‡"** - rozpoznawaÄ‡ obiekty, twarze, gesty
- **"SÅ‚yszeÄ‡"** - rozumieÄ‡ polecenia gÅ‚osowe
- **"MyÅ›leÄ‡"** - podejmowaÄ‡ inteligentne decyzje
- **"UczyÄ‡ siÄ™"** - doskonaliÄ‡ swoje umiejÄ™tnoÅ›ci na podstawie doÅ›wiadczeÅ„

### 1.2 Dlaczego TensorFlow dla robota Unitree G1 EDU-U6?

Robot humanoidalny Unitree G1 EDU-U6 to zaawansowana platforma edukacyjna, ktÃ³ra idealnie nadaje siÄ™ do integracji z TensorFlow:

| MoÅ¼liwoÅ›Ä‡ robota | Zastosowanie TensorFlow | KorzyÅ›Ä‡ |
|------------------|------------------------|---------|
| Kamery HD | Widzenie komputerowe | Rozpoznawanie obiektÃ³w, nawigacja wizualna |
| Mikrofony | Rozpoznawanie mowy | Sterowanie gÅ‚osowe, interakcja z uÅ¼ytkownikiem |
| Manipulatory | Planowanie chwytÃ³w | Inteligentna manipulacja obiektami |
| Sensory | Predykcja ruchu | Bezpieczna nawigacja, unikanie przeszkÃ³d |
| Procesor | Edge computing | Przetwarzanie w czasie rzeczywistym |

---

## 2. GÅ‚Ã³wne zastosowania TensorFlow w projekcie

### 2.1 Widzenie Komputerowe (Computer Vision)

#### A. Rozpoznawanie i klasyfikacja obiektÃ³w

**Technologia:** Konwolucyjne sieci neuronowe (CNN)

**Modele:**
- **Inception V3** - Wysoka dokÅ‚adnoÅ›Ä‡, 1000 kategorii obiektÃ³w
- **MobileNet** - Szybki, optymalny dla robotÃ³w
- **EfficientDet** - Detekcja wielu obiektÃ³w jednoczeÅ›nie

**PrzykÅ‚adowy kod (z tego repozytorium):**
```python
# tensorflow/examples/label_image/label_image.py
graph = load_graph("inception_v3.pb")
image = read_tensor_from_image_file("robot_view.jpg")
results = classify_image(graph, image)
# Robot wie co widzi: "czÅ‚owiek", "piÅ‚ka", "krzesÅ‚o"...
```

**Zastosowania dla G1:**
- Robot rozpoznaje co ma podnieÅ›Ä‡
- Identyfikuje ludzi w pomieszczeniu
- Wykrywa przeszkody na swojej drodze
- Czyta napisy i symbole

**PrzykÅ‚adowy scenariusz:**
```
1. Robot skanuje pomieszczenie kamerÄ…
2. TensorFlow rozpoznaje: "kubek", "ksiÄ…Å¼ka", "telefon"
3. UÅ¼ytkownik mÃ³wi: "podaj mi kubek"
4. Robot lokalizuje kubek, planuje chwyt i podnosi go
```

#### B. Segmentacja obrazu

**Cel:** Rozdzielenie obrazu na regiony (co gdzie jest)

**Zastosowania:**
- Oddzielenie obiektÃ³w od tÅ‚a
- Identyfikacja powierzchni do chodzenia
- Wykrywanie granic przeszkÃ³d

#### C. Åšledzenie obiektÃ³w

**Cel:** Monitorowanie pozycji obiektu w czasie

**Zastosowania:**
- Robot podÄ…Å¼a za osobÄ…
- Åšledzi piÅ‚kÄ™ aby jÄ… zÅ‚apaÄ‡
- Monitoruje ruch w pomieszczeniu

### 2.2 Rozpoznawanie i Przetwarzanie Mowy

#### A. Rozpoznawanie poleceÅ„ gÅ‚osowych (Keyword Spotting)

**Technologia:** Rekurencyjne sieci neuronowe (RNN/LSTM) lub CNN na spektrogramach

**Modele:**
- Speech Commands - Rozpoznawanie krÃ³tkich komend
- DeepSpeech - PeÅ‚na transkrypcja mowy
- Whisper - NajnowoczeÅ›niejszy model od OpenAI

**PrzykÅ‚adowy kod (z tego repozytorium):**
```python
# tensorflow/examples/speech_commands/train.py
# Trening modelu na polskich komendach:
python train.py --wanted_words=idz,stop,lewo,prawo,podnies,poloz

# Rozpoznawanie w czasie rzeczywistym
model = load_model("speech_model.pb")
while robot.is_active():
    audio = microphone.record()
    command = recognize(model, audio)
    robot.execute(command)  # "idÅº", "stop", etc.
```

**Zastosowania dla G1:**
- Sterowanie ruchem: "idÅº naprzÃ³d", "zawrÃ³Ä‡", "stop"
- Kontrola manipulatora: "podnieÅ›", "poÅ‚Ã³Å¼", "otwÃ³rz chwyt"
- Tryby pracy: "tryb autonomiczny", "tryb manualny"
- Interakcja: "tak", "nie", "powtÃ³rz"

**PrzykÅ‚adowy scenariusz:**
```
UÅ¼ytkownik: "G1, podejdÅº do stoÅ‚u"
Robot: Rozpoznaje komendÄ™ "podejdÅº" + obiekt "stÃ³Å‚"
       â†’ UÅ¼ywa widzenia aby znaleÅºÄ‡ stÃ³Å‚
       â†’ Planuje Å›cieÅ¼kÄ™
       â†’ Porusza siÄ™ do celu
```

#### B. Synteza mowy (Text-to-Speech)

**Cel:** Robot mÃ³wi do uÅ¼ytkownika

**Zastosowania:**
- Potwierdzanie poleceÅ„: "Rozumiem, idÄ™ do stoÅ‚u"
- Informowanie o problemach: "Nie mogÄ™ znaleÅºÄ‡ obiektu"
- Raportowanie stanu: "Bateria niska"

### 2.3 Nawigacja i Planowanie ÅšcieÅ¼ki

#### A. SLAM (Simultaneous Localization and Mapping)

**Cel:** Robot buduje mapÄ™ otoczenia i wie gdzie siÄ™ znajduje

**Komponenty TensorFlow:**
- CNN do ekstrahowania cech z obrazÃ³w
- Odometry prediction - przewidywanie ruchu
- Loop closure detection - rozpoznawanie znanego miejsca

**Zastosowania:**
- Autonomiczna eksploracja pomieszczenia
- ZapamiÄ™tywanie rozkÅ‚adu przestrzeni
- Znajdowanie drogi powrotnej

#### B. Unikanie przeszkÃ³d

**Technologia:** Deep Q-Learning, Policy Gradients

**Jak dziaÅ‚a:**
1. Robot widzi otoczenie (kamery + sensory)
2. Model TensorFlow ocenia bezpieczeÅ„stwo rÃ³Å¼nych ruchÃ³w
3. Robot wybiera najbezpieczniejszÄ… akcjÄ™
4. Uczy siÄ™ z doÅ›wiadczenia (Reinforcement Learning)

### 2.4 Manipulacja Obiektami

#### A. Planowanie chwytÃ³w (Grasp Planning)

**Cel:** OkreÅ›lenie jak chwyciÄ‡ obiekt

**Proces:**
1. Rozpoznanie obiektu (co to jest?)
2. Segmentacja (gdzie dokÅ‚adnie jest?)
3. Predykcja punktÃ³w chwytnych
4. Planowanie trajektorii manipulatora
5. Kontrola siÅ‚y chwytu

**Model:** GraspNet, ContactGraspNet

#### B. Kontrola siÅ‚y

**Cel:** Robot nie zniszczy delikatnych obiektÃ³w

**Zastosowania:**
- Podnoszenie jajek bez tÅ‚uczenia
- Podawanie kubka z wodÄ… bez rozlania
- UÅ›cisk dÅ‚oni odpowiedniej siÅ‚y

---

## 3. Architektura Systemu dla Unitree G1

### 3.1 Schemat integracji

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     ROBOT UNITREE G1 EDU-U6                      â”‚
â”‚                                                                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚   PERCEPCJA  â”‚  â”‚  PRZETWARZANIEâ”‚  â”‚      AKCJA           â”‚  â”‚
â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤  â”‚
â”‚  â”‚              â”‚  â”‚              â”‚  â”‚                      â”‚  â”‚
â”‚  â”‚ â€¢ Kamery     â”‚  â”‚ TensorFlow   â”‚  â”‚ â€¢ Manipulatory       â”‚  â”‚
â”‚  â”‚ â€¢ Mikrofony  â”‚â”€>â”‚ Models:      â”‚â”€>â”‚ â€¢ Nogi (chÃ³d)        â”‚  â”‚
â”‚  â”‚ â€¢ IMU        â”‚  â”‚              â”‚  â”‚ â€¢ GÅ‚owa (obrÃ³t)      â”‚  â”‚
â”‚  â”‚ â€¢ LIDAR      â”‚  â”‚ â€¢ Vision     â”‚  â”‚ â€¢ Synteza mowy       â”‚  â”‚
â”‚  â”‚ â€¢ Dotyk      â”‚  â”‚ â€¢ Speech     â”‚  â”‚ â€¢ Sygnalizacja LED   â”‚  â”‚
â”‚  â”‚              â”‚  â”‚ â€¢ Navigation â”‚  â”‚                      â”‚  â”‚
â”‚  â”‚              â”‚  â”‚ â€¢ Control    â”‚  â”‚                      â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚            WARSTWA UCZENIA (Training Pipeline)             â”‚ â”‚
â”‚  â”‚  â€¢ Zbieranie danych z czujnikÃ³w                            â”‚ â”‚
â”‚  â”‚  â€¢ Etykietowanie (labeling)                                â”‚ â”‚
â”‚  â”‚  â€¢ Trening modeli offline                                  â”‚ â”‚
â”‚  â”‚  â€¢ Walidacja i testowanie                                  â”‚ â”‚
â”‚  â”‚  â€¢ Deployment na robota                                    â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    INFRASTRUKTURA ZEWNÄ˜TRZNA                     â”‚
â”‚                                                                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚  GPU Server     â”‚  â”‚  Cloud Storage  â”‚  â”‚  Monitoring     â”‚ â”‚
â”‚  â”‚  (Trening)      â”‚  â”‚  (Modele, dane) â”‚  â”‚  (Dashboard)    â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 3.2 PrzepÅ‚yw danych

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ PRZYKÅADOWY SCENARIUSZ: "Podaj mi butelkÄ™"                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

KROK 1: ROZPOZNAWANIE POLECENIA
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Mikrofon â†’ TensorFlow Speech Model   â”‚
â”‚ Wynik: "podaj" + "butelka"           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
                   â–¼
KROK 2: LOKALIZACJA OBIEKTU
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Kamera â†’ TensorFlow Vision Model     â”‚
â”‚ Wynik: Butelka na stole, (x,y,z)     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
                   â–¼
KROK 3: PLANOWANIE RUCHU
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ TensorFlow + Klasyczne algorytmy     â”‚
â”‚ - ÅšcieÅ¼ka do stoÅ‚u (A*)              â”‚
â”‚ - Trajektoria manipulatora (MoveIt)  â”‚
â”‚ - Punkty chwytne (GraspNet)          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
                   â–¼
KROK 4: WYKONANIE
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Kontrola niskopoziomowa               â”‚
â”‚ - ChÃ³d do stoÅ‚u                       â”‚
â”‚ - WyciÄ…gniÄ™cie rÄ™ki                   â”‚
â”‚ - Chwyt butelki                       â”‚
â”‚ - Podanie uÅ¼ytkownikowi               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
                   â–¼
KROK 5: POTWIERDZENIE
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Robot mÃ³wi: "ProszÄ™"                  â”‚
â”‚ (TensorFlow TTS)                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 4. Praktyczne przykÅ‚ady projektÃ³w

### Projekt 1: Autonomiczny asystent domowy

**Cel:** Robot autonomicznie pomaga w domu

**Funkcje:**
1. Patrzy na stÃ³Å‚ i rozpoznaje przedmioty
2. Reaguje na polecenia: "przynieÅ› telefon", "posprzÄ…ta"j
3. Nawiguje po pomieszczeniu unikajÄ…c przeszkÃ³d
4. Manipuluje obiektami

**Wymagane modele TensorFlow:**
- Object detection (YOLO/EfficientDet)
- Speech recognition (Speech Commands)
- SLAM/navigation
- Grasp planning

### Projekt 2: Robot spoÅ‚eczny (Social Robot)

**Cel:** Robot wchodzi w interakcje z ludÅºmi

**Funkcje:**
1. Rozpoznaje twarze (Face Recognition)
2. Wykrywa emocje (Emotion Detection)
3. Rozmawia z uÅ¼ytkownikiem (Speech + TTS)
4. Reaguje gestami i mimikÄ…

**Wymagane modele TensorFlow:**
- Face detection & recognition
- Emotion classification
- Speech recognition + synthesis
- Gesture recognition

### Projekt 3: Robot edukacyjny

**Cel:** Uczy dzieci poprzez interakcjÄ™

**Funkcje:**
1. Pokazuje karty z obrazkami
2. Pyta "Co to jest?"
3. Rozpoznaje odpowiedÅº mowy
4. Potwierdza lub koryguje

**Wymagane modele TensorFlow:**
- Image classification
- Speech recognition
- Text-to-speech

---

## 5. Workflow: Od pomysÅ‚u do dziaÅ‚ajÄ…cego robota

### Faza 1: Projektowanie (1-2 tygodnie)

```
1. Zdefiniuj funkcjonalnoÅ›Ä‡
   â†“
2. Wybierz modele TensorFlow
   â†“
3. Zaprojektuj architekturÄ™ systemu
   â†“
4. OkreÅ›l wymagania sprzÄ™towe
```

### Faza 2: Przygotowanie danych (2-4 tygodnie)

```
Dla Vision:
- Zbierz zdjÄ™cia z kamery robota
- Etykietuj obiekty (labelimg, CVAT)
- Augmentacja danych

Dla Speech:
- Nagraj komendy (rÃ³Å¼ni mÃ³wcy)
- Transkrypcja
- Augmentacja (szum, echo)
```

### Faza 3: Trening modeli (1-4 tygodnie)

```
1. Transfer learning od pretrained modeli
   â†“
2. Fine-tuning na wÅ‚asnych danych
   â†“
3. Walidacja i optymalizacja
   â†“
4. Konwersja do TensorFlow Lite
```

### Faza 4: Integracja (2-3 tygodnie)

```
1. Implementacja API do robota
   â†“
2. Testowanie na symulatorze
   â†“
3. Deployment na prawdziwym robocie
   â†“
4. Testowanie end-to-end
```

### Faza 5: Optymalizacja (ciÄ…gÅ‚a)

```
- Zbieranie nowych danych w realnych warunkach
- Retraining modeli
- Deployment aktualizacji
- Monitoring performance
```

---

## 6. NarzÄ™dzia i zasoby

### 6.1 Do treningu modeli

| NarzÄ™dzie | Zastosowanie | Link |
|-----------|--------------|------|
| **TensorFlow** | Framework gÅ‚Ã³wny | tensorflow.org |
| **Keras** | API wysokopoziomowe | keras.io |
| **TensorBoard** | Wizualizacja treningu | tensorflow.org/tensorboard |
| **Google Colab** | Darmowe GPU do treningu | colab.research.google.com |

### 6.2 Do zbierania i etykietowania danych

| NarzÄ™dzie | Zastosowanie |
|-----------|--------------|
| **LabelImg** | Etykietowanie obrazÃ³w |
| **CVAT** | Annotacja wideo i obrazÃ³w |
| **Audacity** | Edycja nagraÅ„ audio |
| **RoboFlow** | ZarzÄ…dzanie datasetami |

### 6.3 Gotowe modele (pretrained)

| Model | Zadanie | Å¹rÃ³dÅ‚o |
|-------|---------|--------|
| **MobileNet** | Klasyfikacja obrazÃ³w | TensorFlow Hub |
| **YOLOv5** | Detekcja obiektÃ³w | Ultralytics |
| **Speech Commands** | Komendy gÅ‚osowe | TensorFlow |
| **DeepSpeech** | Rozpoznawanie mowy | Mozilla |

---

## 7. Najlepsze praktyki

### 7.1 WydajnoÅ›Ä‡

âœ… **UÅ¼ywaj TensorFlow Lite** na robocie (3-5x szybsze)
âœ… **Kwantyzacja** modeli (int8) dla mniejszego rozmiaru
âœ… **Batch inference** gdy moÅ¼liwe
âœ… **GPU/TPU** dla treningu, CPU/Edge TPU dla inferencji
âŒ Unikaj peÅ‚nego TensorFlow na robocie (za ciÄ™Å¼ki)

### 7.2 DokÅ‚adnoÅ›Ä‡

âœ… **Transfer learning** od duÅ¼ych modeli pretrained
âœ… **Data augmentation** zwiÄ™ksza robustness
âœ… **Ensemble** modeli dla krytycznych zadaÅ„
âœ… **Continuous learning** z nowych danych
âŒ Nie trenuj od zera jeÅ›li nie musisz

### 7.3 BezpieczeÅ„stwo

âœ… **Fallback** gdy model niepewny (confidence < 0.7)
âœ… **Emergency stop** zawsze dostÄ™pny
âœ… **Sanity checks** na predykcje
âœ… **Human-in-the-loop** dla krytycznych decyzji
âŒ Nigdy nie ufaj modelowi w 100%

---

## 8. Troubleshooting - CzÄ™ste problemy

### Problem: Model dziaÅ‚a wolno na robocie

**RozwiÄ…zania:**
1. Konwertuj do TensorFlow Lite
2. UÅ¼yj kwantyzacji (int8)
3. Wybierz mniejszy model (MobileNet zamiast ResNet)
4. Zmniejsz rozdzielczoÅ›Ä‡ wejÅ›ciowÄ…
5. UÅ¼yj Edge TPU jeÅ›li dostÄ™pny

### Problem: Niska accuracy

**RozwiÄ…zania:**
1. Zbierz wiÄ™cej danych treningowych
2. Popraw jakoÅ›Ä‡ etykiet
3. UÅ¼yj data augmentation
4. WyprÃ³buj wiÄ™kszy/lepszy model
5. Fine-tuning dÅ‚uÅ¼ej

### Problem: Model siÄ™ "przeuczyÅ‚" (overfitting)

**RozwiÄ…zania:**
1. ZwiÄ™ksz zbiÃ³r treningowy
2. UÅ¼yj regularizacji (L2, dropout)
3. Data augmentation
4. Early stopping podczas treningu
5. Uproszczenie modelu

---

## 9. Podsumowanie

### Kluczowe wnioski

1. **TensorFlow to potÄ™Å¼ne narzÄ™dzie** dla robotyki edukacyjnej
2. **Unitree G1 EDU-U6** idealnie nadaje siÄ™ do projektÃ³w z AI
3. **Gotowe modele** pozwalajÄ… szybko startowaÄ‡
4. **WÅ‚asne dane** dajÄ… najlepsze rezultaty dla specific tasks
5. **Praktyka** jest kluczowa - eksperymentuj!

### Co dalej?

#### Dla poczÄ…tkujÄ…cych:
1. âœ… PrzejdÅº przez przykÅ‚ady w `tensorflow/examples/`
2. âœ… Uruchom label_image.py z wÅ‚asnymi zdjÄ™ciami
3. âœ… Wytrenuj model speech_commands na polskich sÅ‚owach
4. âœ… Zintegruj z symulatorem robota

#### Dla Å›rednio zaawansowanych:
1. âœ… Zbierz wÅ‚asny dataset z kamery robota
2. âœ… Wytrenuj model detekcji obiektÃ³w
3. âœ… Zaimplementuj prosty SLAM
4. âœ… StwÃ³rz kompletny system sterowania gÅ‚osem

#### Dla zaawansowanych:
1. âœ… Reinforcement Learning do kontroli robota
2. âœ… Multi-task learning (vision + speech jednoczeÅ›nie)
3. âœ… Real-time SLAM z deep learning
4. âœ… Publish paper o swoich wynikach!

---

## 10. Dodatkowe zasoby

### Dokumentacja w tym repozytorium (w jÄ™zyku polskim):
- `README_PL.md` - OgÃ³lny przewodnik TensorFlow
- `UNITREE_G1_GUIDE_PL.md` - SzczegÃ³Å‚owy przewodnik dla robota G1
- `tensorflow/examples/label_image/README_PL.md` - Rozpoznawanie obrazÃ³w
- `tensorflow/examples/speech_commands/README_PL.md` - Rozpoznawanie mowy

### Kursy online:
- TensorFlow w praktyce (Coursera)
- Deep Learning Specialization (Coursera)
- Fast.ai - Practical Deep Learning for Coders

### SpoÅ‚ecznoÅ›ci:
- TensorFlow Forum: discuss.tensorflow.org
- r/MachineLearning (Reddit)
- r/robotics (Reddit)
- Lokalna grupa AI/ML na Politechnice Rzeszowskiej

### KsiÄ…Å¼ki (polecane):
- "Hands-On Machine Learning" - AurÃ©lien GÃ©ron
- "Deep Learning" - Ian Goodfellow
- "Programming Robots with ROS" - Morgan Quigley

---

**Å»yczymy powodzenia w Waszych projektach z robotem Unitree G1 EDU-U6!**

**ZespÃ³Å‚ Laboratorium AI i Robotyki**
**Politechnika Rzeszowska**

ğŸ¤– + ğŸ§  = ğŸš€
